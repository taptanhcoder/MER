{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c6f920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Environment & quick sanity\n",
    "import os, sys, platform, torch, numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "# Nếu notebook ở project root: thêm \"src\" vào sys.path\n",
    "if not (Path.cwd() / \"configs\").exists():\n",
    "    sys.path.append(str(Path.cwd() / \"src\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e708598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Reload updated modules\n",
    "import importlib\n",
    "\n",
    "import configs.base as base_cfg\n",
    "importlib.reload(base_cfg)\n",
    "from configs.base import Config\n",
    "\n",
    "import loading.dataloader as data_loader\n",
    "importlib.reload(data_loader)\n",
    "from loading.dataloader import build_train_test_dataset\n",
    "\n",
    "import model.networks as networks\n",
    "importlib.reload(networks)\n",
    "from model.networks import MER  # (MemoCMT = MER alias có trong networks.py)\n",
    "\n",
    "import model.losses as losses_mod\n",
    "importlib.reload(losses_mod)\n",
    "from model.losses import get_loss\n",
    "\n",
    "import training.trainer as trainer_mod\n",
    "importlib.reload(trainer_mod)\n",
    "from training.trainer import TorchTrainer\n",
    "\n",
    "import training.callbacks as cbs_mod\n",
    "importlib.reload(cbs_mod)\n",
    "from training.callbacks import CheckpointsCallback\n",
    "\n",
    "import training.optimizers as opt_mod\n",
    "importlib.reload(opt_mod)\n",
    "from training.optimizers import split_param_groups, build_optimizer\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup, BatchEncoding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c227be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Config — đã đồng bộ với code mới (num_classes=5, momentum, bucketing/sampler options)\n",
    "cfg = Config(\n",
    "    name=\"MER_VNEMOS_maskfix_stable\",\n",
    "    checkpoint_dir=\"../checkpoints/mer_vnemos_maskfix_stable\",\n",
    "\n",
    "    # Train runtime\n",
    "    num_epochs=30,\n",
    "    batch_size=8,\n",
    "    num_workers=2,\n",
    "\n",
    "    # LR / optim (head lr set tại param_groups)\n",
    "    learning_rate=2e-5,\n",
    "    optimizer_type=\"AdamW\",\n",
    "    adam_weight_decay=0.01,\n",
    "\n",
    "    # Scheduler\n",
    "    scheduler_type=\"cosine_warmup\",\n",
    "    warmup_ratio=0.05,\n",
    "    scheduler_step_unit=\"step\",\n",
    "\n",
    "    # Loss (chọn 1: \"FocalLoss\" hoặc \"LabelSmoothingCE\")\n",
    "    loss_type=\"LabelSmoothingCE\",\n",
    "    label_smoothing=0.05,\n",
    "\n",
    "    # Data\n",
    "    data_root=\"../output\",\n",
    "    jsonl_dir=\"\",\n",
    "    sample_rate=16000,\n",
    "    max_audio_sec=None,           # không crop cứng\n",
    "    text_max_length=96,\n",
    "\n",
    "    # Bucketing & Sampler chống length-bias\n",
    "    use_length_bucket=True,\n",
    "    length_bucket_size=64,\n",
    "    bucketing_text_alpha=0.03,\n",
    "    use_weighted_sampler=True,    # nếu bật length bucket, sampler sẽ dùng cho nhánh 'else'\n",
    "    lenfreq_alpha=0.5,\n",
    "\n",
    "    # Model\n",
    "    model_type=\"MemoCMT\",\n",
    "    text_encoder_type=\"phobert\",\n",
    "    text_encoder_ckpt=\"vinai/phobert-base\",\n",
    "    text_encoder_dim=768,\n",
    "    text_unfreeze=False,\n",
    "\n",
    "    audio_encoder_type=\"wav2vec2_xlsr\",\n",
    "    audio_encoder_ckpt=\"facebook/wav2vec2-large-xlsr-53\",\n",
    "    audio_encoder_dim=1024,\n",
    "    audio_unfreeze=False,\n",
    "\n",
    "    fusion_dim=768,\n",
    "    fusion_head_output_type=\"cls\",     # khuyến nghị: \"cls\" hoặc \"mean\"\n",
    "    linear_layer_output=[256, 128],\n",
    "    dropout=0.10,\n",
    "\n",
    "    # Tricks\n",
    "    use_amp=True,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # Gradual Unfreeze (có thể bật sau)\n",
    "    gradual_unfreeze_epoch=3,\n",
    "    text_unfreeze_last_k=4,\n",
    "    audio_unfreeze_last_k=4,\n",
    "\n",
    "    # Checkpoints\n",
    "    save_best_val=True,\n",
    "    max_to_keep=2,\n",
    ")\n",
    "\n",
    "print(\"Checkpoint dir:\", Path(cfg.checkpoint_dir).resolve())\n",
    "base_dir = (Path(cfg.data_root) / (cfg.jsonl_dir or \"\")).resolve()\n",
    "print(\"base_dir:\", base_dir)\n",
    "print(\"audio_root (auto):\", Path(getattr(cfg, \"audio_root\", base_dir.parent)).resolve())\n",
    "for fn in [\"train.jsonl\",\"valid.jsonl\",\"test.jsonl\"]:\n",
    "    print(fn, (base_dir/fn).exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3053d1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Build loaders bằng builder mới (đã có bucketing mix và audio_attn_mask)\n",
    "train_loader, eval_loader = build_train_test_dataset(cfg)\n",
    "\n",
    "# Lấy id2label từ dataset train (builder nội bộ đã cố định label2id trên train)\n",
    "# Quick peek để suy ra nhãn:\n",
    "from loading.dataloader import VNEMOSDataset\n",
    "tmp_ds = VNEMOSDataset(cfg, \"train\")\n",
    "label2id = tmp_ds.label2id\n",
    "id2label = [k for k, v in sorted(label2id.items(), key=lambda x: x[1])]\n",
    "cfg.num_classes = len(id2label)\n",
    "\n",
    "print(\"Classes:\", cfg.num_classes, id2label)\n",
    "print(f\"Train batches: {len(train_loader)} | Eval batches: {len(eval_loader) if eval_loader else 0}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Sanity: kiểm tra mask & encoded lengths từ 1 batch (dùng meta[\"audio_attn_mask\"])\n",
    "import torch\n",
    "from model.modules import build_audio_encoder\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "((tok_sc, audio_sc, labels_sc), meta_sc) = batch\n",
    "device_tmp = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "attn_mask_audio_input = meta_sc.get(\"audio_attn_mask\", None)\n",
    "if attn_mask_audio_input is None:\n",
    "    # fallback (không cần nếu collator mới): dựng từ lengths\n",
    "    lengths = torch.tensor(meta_sc[\"audio_lengths\"], device=device_tmp, dtype=torch.long)\n",
    "    B, T = audio_sc.shape\n",
    "    attn_mask_audio_input = (torch.arange(T, device=device_tmp).unsqueeze(0) < lengths.unsqueeze(1)).long()\n",
    "else:\n",
    "    attn_mask_audio_input = attn_mask_audio_input.to(device_tmp)\n",
    "\n",
    "audio_enc = build_audio_encoder(cfg).to(device_tmp).eval()\n",
    "with torch.no_grad():\n",
    "    a_feat = audio_enc(audio_sc.to(device_tmp), attention_mask=attn_mask_audio_input)  # (B, L_a, D)\n",
    "    if \"audio_lengths\" in meta_sc:\n",
    "        lengths = torch.tensor(meta_sc[\"audio_lengths\"], device=device_tmp, dtype=torch.long)\n",
    "        L_valid = audio_enc.get_feat_lengths(lengths).clamp(min=1, max=a_feat.size(1))\n",
    "        kpm_audio = (torch.arange(a_feat.size(1), device=device_tmp).unsqueeze(0) >= L_valid.unsqueeze(1))\n",
    "        any_all_masked = bool(kpm_audio.all(dim=1).any().item())\n",
    "    else:\n",
    "        any_all_masked = False\n",
    "\n",
    "print(\"Sanity | audio input:\", tuple(audio_sc.shape), \"| encoded len L_a:\", a_feat.size(1))\n",
    "if \"audio_lengths\" in meta_sc:\n",
    "    print(\"Sanity | first 8 L_valid:\", L_valid[:8].tolist())\n",
    "print(\"Sanity | Any sample all-masked? ->\", any_all_masked)\n",
    "\n",
    "del audio_enc\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acb231b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Trainer “nhẹ” cho notebook — giữ đúng cách chuyển BatchEncoding thành **kwargs\n",
    "import torch\n",
    "from transformers import BatchEncoding\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "network = MER(cfg, device=device).to(device)\n",
    "criterion = get_loss(cfg)\n",
    "\n",
    "class MERNotebookTrainer(TorchTrainer):\n",
    "    def __init__(self, cfg, network, criterion, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.cfg = cfg\n",
    "        self.network = network\n",
    "        self.criterion = criterion\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.network.to(self.device)\n",
    "        self.use_amp = bool(getattr(cfg, \"use_amp\", torch.cuda.is_available()))\n",
    "        self.max_grad_norm = float(getattr(cfg, \"max_grad_norm\", 0.0))\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=self.use_amp)\n",
    "\n",
    "    def _to_device_text(self, x):\n",
    "        if isinstance(x, BatchEncoding):\n",
    "            x = x.to(self.device)\n",
    "            return {k: v for k, v in x.items()}\n",
    "        if isinstance(x, dict):\n",
    "            return {k: v.to(self.device) for k, v in x.items()}\n",
    "        return x.to(self.device)\n",
    "\n",
    "    def _standardize_batch(self, batch):\n",
    "        # batch = ((tok, audio, labels), meta)\n",
    "        if isinstance(batch, (tuple, list)) and len(batch) == 2:\n",
    "            a, meta = batch\n",
    "            if isinstance(a, (tuple, list)) and len(a) == 3:\n",
    "                tok, audio, labels = a\n",
    "                return tok, audio, labels, meta\n",
    "        raise ValueError(f\"Batch structure không hỗ trợ: preview={str(batch)[:200]}\")\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        self.network.train()\n",
    "        self.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        (tok, audio, labels, meta) = self._standardize_batch(batch)\n",
    "        audio  = audio.to(self.device, non_blocking=True)\n",
    "        labels = labels.to(self.device, non_blocking=True)\n",
    "        tok    = self._to_device_text(tok)\n",
    "\n",
    "        if self.use_amp:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = self.network(tok, audio, meta=meta)\n",
    "                logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "                loss = self.criterion(out, labels) if isinstance(out, (tuple, list)) else self.criterion(logits, labels)\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if self.max_grad_norm and self.max_grad_norm > 0:\n",
    "                self.scaler.unscale_(self.optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "        else:\n",
    "            out = self.network(tok, audio, meta=meta)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            loss = self.criterion(out, labels) if isinstance(out, (tuple, list)) else self.criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            if self.max_grad_norm and self.max_grad_norm > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "            self.optimizer.step()\n",
    "\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = (preds == labels).float().mean()\n",
    "        return {\"loss\": float(loss.detach().cpu()), \"acc\": float(acc.detach().cpu())}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        self.network.eval()\n",
    "        (tok, audio, labels, meta) = self._standardize_batch(batch)\n",
    "        audio  = audio.to(self.device, non_blocking=True)\n",
    "        labels = labels.to(self.device, non_blocking=True)\n",
    "        tok    = self._to_device_text(tok)\n",
    "        with torch.no_grad():\n",
    "            out = self.network(tok, audio, meta=meta)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            loss = self.criterion(out, labels) if isinstance(out, (tuple, list)) else self.criterion(logits, labels)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            acc = (preds == labels).float().mean()\n",
    "        return {\"val_loss\": float(loss.detach().cpu()),\n",
    "                \"val_acc\": float(acc.detach().cpu()),\n",
    "                \"preds\": preds.detach().cpu(),\n",
    "                \"targets\": labels.detach().cpu()}\n",
    "\n",
    "trainer = MERNotebookTrainer(cfg, network, criterion, log_dir=\"logs\")\n",
    "print(\"Trainer ready. Device:\", trainer.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf77d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Optimizer (tách LR: encoder thấp, head cao) + Scheduler cosine warmup\n",
    "enc_lr  = 5e-6\n",
    "head_lr = 2e-5\n",
    "\n",
    "param_groups = split_param_groups(trainer, lr_enc=enc_lr, lr_head=head_lr, weight_decay=0.01)\n",
    "optimizer = build_optimizer(\"adamw\", param_groups, lr=head_lr, weight_decay=0.01)\n",
    "\n",
    "total_steps = len(train_loader) * cfg.num_epochs\n",
    "warmup_steps = max(1, int(cfg.warmup_ratio * total_steps))\n",
    "print(\"Total steps:\", total_steps, \"| Warmup steps:\", warmup_steps)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=warmup_steps,\n",
    "    num_training_steps=total_steps,\n",
    ")\n",
    "\n",
    "trainer.compile(\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    lr=head_lr,\n",
    "    param_groups=None,\n",
    "    scheduler_step_unit=cfg.scheduler_step_unit,  \n",
    ")\n",
    "print(\"Param groups:\", len(optimizer.param_groups))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ef96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Callbacks — monitor macro-F1 (TorchTrainer của bạn cần tính dựa trên preds/targets)\n",
    "ckpt_cb = CheckpointsCallback(\n",
    "    cfg.checkpoint_dir,\n",
    "    save_freq=200,\n",
    "    max_to_keep=cfg.max_to_keep,\n",
    "    save_best_val=True,\n",
    "    monitor=\"val_macro_f1\",  # đảm bảo TorchTrainer tổng hợp macro-F1 từ preds/targets\n",
    "    mode=\"max\",\n",
    ")\n",
    "callbacks = [ckpt_cb]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8dba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Train\n",
    "trainer.fit(train_loader, epochs=cfg.num_epochs, eval_data=eval_loader, callbacks=callbacks)\n",
    "print(\"Best checkpoint:\", getattr(ckpt_cb, \"best_path\", \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Đánh giá & báo cáo\n",
    "best_path = getattr(ckpt_cb, \"best_path\", \"\")\n",
    "if isinstance(best_path, str) and best_path.endswith(\".pth\") and Path(best_path).exists():\n",
    "    print(\"Loading best weights for eval:\", best_path)\n",
    "    state = torch.load(best_path, map_location=trainer.device)\n",
    "    trainer.network.load_state_dict(state)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def collect_preds(trainer, loader):\n",
    "    all_preds, all_labels = [], []\n",
    "    trainer.network.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            (tok, audio, labels, meta) = trainer._standardize_batch(batch)\n",
    "            audio  = audio.to(trainer.device)\n",
    "            labels = labels.to(trainer.device)\n",
    "            tok    = trainer._to_device_text(tok)\n",
    "\n",
    "            out = trainer.network(tok, audio, meta=meta)\n",
    "            logits = out[0] if isinstance(out, (tuple, list)) else out\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    return torch.cat(all_preds), torch.cat(all_labels)\n",
    "\n",
    "if eval_loader is not None:\n",
    "    preds, labels = collect_preds(trainer, eval_loader)\n",
    "    num_classes = cfg.num_classes\n",
    "\n",
    "    cm = torch.zeros((num_classes, num_classes), dtype=torch.int32)\n",
    "    for t, p in zip(labels, preds):\n",
    "        cm[t.long(), p.long()] += 1\n",
    "\n",
    "    per_class_acc = (cm.diag().float() / cm.sum(dim=1).clamp(min=1).float()).numpy()\n",
    "    overall_acc = (preds == labels).float().mean().item()\n",
    "\n",
    "    print(\"Overall Acc: %.4f\" % overall_acc)\n",
    "    print(\"Per-class Acc:\")\n",
    "    for i, acc in enumerate(per_class_acc):\n",
    "        name = id2label[i] if i < len(id2label) else str(i)\n",
    "        print(f\"  {i} ({name}): {acc:.4f}\")\n",
    "    print(\"Confusion Matrix:\\n\", cm.numpy())\n",
    "\n",
    "    try:\n",
    "        from sklearn.metrics import classification_report\n",
    "        target_names = [id2label[i] for i in range(num_classes)]\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(labels.numpy(), preds.numpy(), target_names=target_names, digits=4))\n",
    "    except Exception as e:\n",
    "        print(\"sklearn not available:\", e)\n",
    "else:\n",
    "    print(\"No eval_loader available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a4e272",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Save model weights to HDF5 (.h5)\n",
    "import json, h5py\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Đảm bảo đã train xong và trainer.network đang giữ trọng số tốt nhất (nếu bạn đã load best trước đó).\n",
    "h5_path = Path(cfg.checkpoint_dir) / \"model_weights.h5\"\n",
    "\n",
    "state = trainer.network.state_dict()  # OrderedDict[str, Tensor]\n",
    "\n",
    "with h5py.File(h5_path, \"w\") as f:\n",
    "    # --- Metadata ---\n",
    "    f.attrs[\"model_name\"] = \"MER\"\n",
    "    f.attrs[\"num_classes\"] = int(cfg.num_classes)\n",
    "    f.attrs[\"id2label\"] = json.dumps(id2label, ensure_ascii=False)\n",
    "    f.attrs[\"torch_version\"] = torch.__version__\n",
    "\n",
    "    # Lưu toàn bộ cấu hình (serialize về string để an toàn)\n",
    "    cfg_group = f.create_group(\"config\")\n",
    "    for k, v in vars(cfg).items():\n",
    "        try:\n",
    "            cfg_group.attrs[k] = json.dumps(v, ensure_ascii=False)\n",
    "        except TypeError:\n",
    "            cfg_group.attrs[k] = str(v)\n",
    "\n",
    "    # --- Weights ---\n",
    "    weights_group = f.create_group(\"state_dict\")\n",
    "    for name, tensor in state.items():\n",
    "        arr = tensor.detach().cpu().numpy()\n",
    "        # h5py tạo dataset theo tên tham số; thay dấu chấm bằng gạch dưới nếu bạn muốn phẳng\n",
    "        dset = weights_group.create_dataset(name, data=arr)\n",
    "        dset.attrs[\"shape\"] = arr.shape\n",
    "        dset.attrs[\"dtype\"] = str(arr.dtype)\n",
    "\n",
    "print(f\"Saved HDF5 weights to: {h5_path.resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
